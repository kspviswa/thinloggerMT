{"name":"ThinloggerMT","tagline":"Lock-free, Thread Based Log Buffer Implementation, for Debugging High Performance Multi-threaded Application","body":"#Introduction\r\n\r\nI would like to present a concept & implementation about lock-free, thread based log buffer instrumentation in multi-threaded applications. Please post your feedback on comments, if any.\r\n\r\n#Background\r\n\r\nWhen multiple threads work on a single process, certain scenarios will occur, which we term as race condition. During race condition, the output of execution turns unpredictable and most importantly, these race conditions are always inconsistent.\r\n\r\nDue to the inconsistent nature for race conditions, debugging becomes much tougher and challenging.\r\nThe most traditional way to deal with such situations is to log relevant details to a log file, which can be examined after some time, or, do a live debugging.\r\n\r\n**Live debugging** can be done, only if you guys are lucky Smile | :) . Mostly, bugs due to race conditions are hard to reproduce and difficult to find root-cause.\r\n\r\nIn case of issues, raised from production site, engineers have less chances to keep their hands-on the problematic site.\r\n\r\nWith the provided opportunities, somehow, the puzzle has to be solved.\r\n\r\nWith the learnings, that I got from, support experience, I would like to share a novel way with which we can log the debugging information, that too for a high performance, race conditions.\r\n\r\n#Problems with Traditional Methods\r\n\r\n##File logging\r\n\r\nThis is one of the easiest & simplest methods. But file I/O will significantly add lot of delay. A delay even in the range of milli seconds, will impact the race condition, so that, the problematic scenario won’t even occur.\r\n\r\n##Common log structure logging\r\n\r\nThis is much dev approach, where-in, a global log buffer, is used to log relevant details to that buffer. However, this buffer again becomes a shared resource b/w threads. So obviously there has to be a mutex to guard the shared resource. When you bring in mutual exclusion, then you are breaking the parallelism. This again, brings in latency, introduced by locking & unlocking of mutexes, thereby forcing the race-condition, not to occur at all.\r\n\r\n#Concept & Implementation\r\n\r\nAs you can guess, the problem with common log-structure logging is the presence of shared mutex lock.\r\n\r\nAs long as one or more threads, is using the same resource, lock is necessary. So we have to break this common resource.\r\n\r\nIf all the working threads are having their buffer to log, what they do currently and if there is a global place in the process which will connect all these unique buffers, then that will exactly serves the purpose.\r\n\r\nThat’s the idea behind this article. This can be achieved in 2 ways. Dynamically allocate a buffer to the incoming thread [as threads can be spawned dynamically] or keep the thread id as index of global buffers. While the first option is most scalable option, the second option can be opted, if the latency introduced by attaching a buffer to the thread is also crucial for your scenario.\r\n\r\n`gGLobalBuffers` is the array of log buffers, indexed by thread ids. Here you can ask about the wastage of buffers for unused threads. My argument would be, this is not going to be a feature or customer deliverable. We can do anything that it takes to solve a customer issue. So sacrificing some memory to find the root-cause is still a good attempt.\r\n\r\nEvery buffer is a type of `THINLOGTHREADCONTAINER`, which contains another array of the log buffer of type THINLOGBUFFER and the thread id index to the globalBuffer.\r\nThere are chances that the same thread is being called by multiple callers [consider worker threads]. In such case, if every thread has only single buffer, then the next caller of same thread will over-write the information, that was recorded by the previous caller for same thread. We don’t want that to happen, do we Smile | :) . That’s the reason for tagging multiple log buffers to a single thread container.\r\n\r\nFollowing figure depicts this.\r\n\r\n![](https://i2.wp.com/www.codeproject.com/KB/debug/824316/DesignFlow.jpg)\r\n\r\n###Special container – Thread 0\r\n\r\nFirst container is special. Obviously, you will not have a thread 0. So this is used as a overflow container. For example, you had estimated that, for your application, you are going to have only 100 threads. So 100 containers might be enough for you. But during run time, another thread pokes-in [like customer had changed the config during run-time]. Thread 0 container, comes for rescue at that time. The presence of data on container 0 tells that you need to revisit your resource settings.\r\n\r\n###Using the Code\r\n\r\n**STEP 1 – DEFINE YOUR OWN INSTRUMENTATION INFO IN THINLOGBUFFER STRUCT IN THINLOGGERMT.H.**\r\n\r\n```\r\n    struct thinLogBuffer\r\n    {\r\n        unsigned long nTestLong;\r\n        int nTestInt;\r\n        float nTestFloat;\r\n        char szTestChar[MAX_CHAR];\r\n        unsigned long nClockTicks;\r\n    };\r\n\r\n    typedef struct thinLogBuffer THINLOGBUFFER;\r\n```\r\n\r\nTry to live with primitives. The instrumentation logic, simply uses `memcpy()`. So copying the pointer, will not copy the actual pointee. But again, you can override the `doInstrumentation()` to perform a deep copy.\r\n\r\n**STEP 2 – DEFINE YOUR TOTAL CONTAINER SIZE & NUMBER OF BUFFERS PER CONTAINER**\r\n\r\n```\r\n    #define MAX_BUFFERS_PER_THREAD 5\r\n    #define MAX_THREADS_PER_PROCESS 7000\r\n```\r\nThe above snippet will make 7000 containers [which will cater for threads whose IDs fall between 0 – 7000] and each thread can save 5 callers information simultaneously.\r\n\r\n**Important Note**\r\n\r\n_In case of Solaris, the threads start from thread 1 and go on. In case of Linux, there is no concept of threads / lwp. These are simple processes, but with shared address spaces. So in case of Linux, the thread ids will keep increasing depending on the number of processes running during that time. If you are using this solution in Linux, you should be increasing the number of containers, as the lwp id of each thread will be used as index to global buffers. You can also maintain a application specific logic of mapping a short integers to lwps and give that short integer as a index to globalBuffers. But again, this may introduce latency. If your application is not that much real-time, then you can go for it._\r\n\r\n**STEP 3 – USE THE APIS INIT() & DOINSTRUMENTATION**\r\n\r\nInit the library using `init()`\r\n\r\n```\r\n       int nRet = init();\r\n\r\n        if(nRet < 0)\r\n        {\r\n                // probably some problem\r\n                printf(\"\\n Unable to init the library\");\r\n                return;\r\n        }\r\n```\r\n\r\nCreate a local `THINLOGBUFFER`, fill the values and do the instrumentation.\r\n\r\n```\r\n        THINLOGBUFFER myBuffer;\r\n\r\n        struct timeval tp;\r\n        gettimeofday(&tp, NULL);\r\n        unsigned long ms = tp.tv_sec * 1000 + tp.tv_usec / 1000;\r\n\r\n        myBuffer.nClockTicks = ms;\r\n        myBuffer.nTestFloat = (676.56 + rand());\r\n        myBuffer.nTestInt = rand();\r\n        myBuffer.nTestLong = rand() + myBuffer.nTestFloat;\r\n        strcpy(myBuffer.szTestChar, \"Viswaaaaaa\");\r\n\r\n        int nRet = doInstrumentation(nThreadID, &myBuffer);\r\n\r\n        if(nRet < 0)\r\n        {\r\n                printf(\"Instrumentation failed\");\r\n        }\r\n```\r\n\r\n_Note: You can call this `doInstrumentation()` for upto `MAX_BUFFERS_PER_THREAD` times from the same thread. You can still call `N` number of times, just that, it will overwrite the buffers and when it overflows the `MAX_BUFFERS_PER_THREAD limit`_\r\n\r\n##Reading the Buffers in Post-mortem Debugging\r\nAs you can rightly guess, these buffers are in global location of process. If a problem / crash happens in site, and if the customer provides the core / crashdump from the site, with this instrumentation, then debugging will be as easy, like this:\r\n\r\n```\r\n    (gdb) print gGLobalBuffers[5313]\r\n    $6 = {\r\n         nLookupId = 0, \r\n         theRingBuffers = {{\r\n         nTestLong = 2138875903, \r\n         nTestInt = 1957747793, \r\n         nTestFloat = 1.71463757e+09, \r\n         szTestChar = Viswaaaaaa00\\267\\224\\337w\\267, 00\r\n         <repeats 12 times>, \\330]\\267, 00 <repeats 13 times>, y\\267, \r\n         nClockTicks = 3084490601\r\n        }, \r\n        \r\n```\r\n\r\n#Points of Interest\r\n\r\nThinloggerMT is a simple ringle-buffer implementation, where the buffers are indexed with the thread-id. i.e Each thread has its own log buffer and these buffers are in-turn related to each other.\r\n\r\nSince each thread has its own buffer, these threads don’t depend on each other. Hence there is no need for the threads to wait for a lock, thereby increasing the log-time of the threads.\r\n\r\nThese buffers are maintained in global state. As of now, if process core-dumps, then these buffers could be examined from the core-file. Future works have been planned, to dump the instrumentation on-demand basis or shared memory. This buffer has been provided in a static lib format.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}